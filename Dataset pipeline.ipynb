{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b4fa2c",
   "metadata": {},
   "source": [
    "# Glacier Segmentation - Dataset pipeline\n",
    "\n",
    "This notebook contains the code that we used to create the dataset for our project.\n",
    "\n",
    "This notebook also assumes that you have a Google Earth Engine account, ready to be used with external APIs and that you have a project where the shapefile of the glacier is hosted as a ressource (glamos/SIG_2016_wgs84).\n",
    "\n",
    "To be ran, you will need the following libraries (not in the requirements as not needed for inference):\n",
    "\n",
    " - geopandas\n",
    " - shapely\n",
    " - rasterio\n",
    " - earthengine-api\n",
    "\n",
    " ## Library imports and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb9c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gid', 'pk_glacier', 'sgi-id', 'name', 'rl_0', 'rl_1', 'rl_2', 'rl_3',\n",
      "       'i_code', 'year_acq', 'year_rel', 'area_km2', 'length_km', 'masl_min',\n",
      "       'masl_med', 'masl_mean', 'masl_max', 'slope_deg', 'aspect_deg',\n",
      "       'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil \n",
    "# Geospatial libraries\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "import geemap\n",
    "import ee\n",
    "\n",
    "#PATHs\n",
    "shape_path_2016 = \"glamos/2016/SGI_2016_glaciers.shp\"\n",
    "sgi_2016 = gpd.read_file(shape_path_2016)\n",
    "\n",
    "# Initialize the Earth Engine module\n",
    "try:\n",
    "    ee.Initialize(project=\"b3testdrive\") # this is the name of our Google Cloud project\n",
    "except Exception:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project=\"b3testdrive\")\n",
    "\n",
    "# Attributes of the shape file\n",
    "print(\"available fields in the 2016 glacier inventory:\", sgi_2016.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34fa07",
   "metadata": {},
   "source": [
    "GoogleEarthEngine **only allows uploading Shapefile in WSG84**, so we need to convert the LV95 ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df735360",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sgi_2016.crs is None: \n",
    "    sgi_2016.set_crs(epsg=2056, inplace=True) # Ensure that it has the correct LV95 CRS\n",
    "sgi_2016_wgs84 = sgi_2016.to_crs(epsg=4326) # Save a copy\n",
    "sgi_2016_wgs84.to_file(\"glamos/SGI_2016_wgs84.shp\")\n",
    "print(\"Reprojected glacier shapefile saved to 'glamos/SGI_2016_wgs84.shp', please upload it to Earth Engine assets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4825e54",
   "metadata": {},
   "source": [
    "## Downloading tiles in the Area of Interest\n",
    "\n",
    "The system is quite simple! We have the geometry of each individual glacier, and for each of those glacier, we want to have square patches that covers them -> This is what we do here, with tiles of 224x224 of size and a resolution of 10m.\n",
    "\n",
    "Additionally, we verify that the tile still has at list a minimum of ~50 pixels in it that are glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gdf) = 1400\n",
      "crs = EPSG:2056\n",
      "total_bounds = [2552348.6644433  1078863.87018848 2827545.1764972  1235126.54123004]\n",
      "example year_acq unique (head) = [2014, 2016, 2016, 2014, 2016]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Configuration for the data acquisition\n",
    "TILE_SIZE_PX = 224\n",
    "RESOLUTION = 10 \n",
    "TILE_SIZE_M = TILE_SIZE_PX * RESOLUTION  # 2240m\n",
    "OUTPUT_DIR = \"dataset/images_raw_2056\" # Output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Filters\n",
    "MIN_INTERSECTION_M2 = 5000 # Minimum intersection area between glacier. Arbitrary just to avoid extremly small noise (approximately ensures at leasst a 7x7 pixel area)\n",
    "MIN_YEAR = 2015 # since sent2 started in mid 2015\n",
    "\n",
    "# Loaded into EPSG:2056 (==LV95)\n",
    "gdf = gpd.read_file(\"glamos/SGI_2016_wgs84.shp\") \n",
    "if gdf.crs.to_epsg() != 2056: \n",
    "    gdf = gdf.to_crs(epsg=2056)\n",
    "\n",
    "def get_sentinel_raw(year):\n",
    "    \"\"\"\n",
    "    Fetch raw Sentinel-2 data (reflectance) for the given year and summer months (July to September).\n",
    "    Uses median composite to reduce cloud cover.\n",
    "    \n",
    "    Args:\n",
    "        year (int): Year of acquisition.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = f'{year}-07-01'\n",
    "    end = f'{year}-09-30'\n",
    "    \n",
    "    collection = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "        .filterDate(start, end) \\\n",
    "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
    "        .median() # Median to reduce clouds, Maximum 20% cloud cover\n",
    "    \n",
    "    # Blue, Green, Red, NIR (NIR not used in the end, but still we get it)\n",
    "    return collection.select(['B2', 'B3', 'B4', 'B8'])\n",
    "\n",
    "# Stats\n",
    "stats = { \"tiles_kept\": 0, \"errors\": 0 }\n",
    "\n",
    "\n",
    "print(f\"Début du traitement en EPSG:2056...\")\n",
    "\n",
    "# We iterate over all the glaciers defined in the inventory shapefile\n",
    "for index, row in gdf.iterrows():\n",
    "    g_id = row['pk_glacier']\n",
    "    year = int(row[\"year_acq\"]) # Acquisition year -> to have an image as close as possible to the inventory date\n",
    "    \n",
    "    if year < MIN_YEAR: continue # Filter for Sentinel-2 availability\n",
    "\n",
    "    # Geometry (LV95)\n",
    "    geom_shapely = row.geometry\n",
    "    minx, miny, maxx, maxy = geom_shapely.bounds\n",
    "    \n",
    "    # Grid coordinates (To cover the full size of the glacier)\n",
    "    x_coords = np.arange(minx, maxx, TILE_SIZE_M)\n",
    "    y_coords = np.arange(miny, maxy, TILE_SIZE_M)\n",
    "    \n",
    "    # Local counter = tile number for the current glacier -> we iterate over all the tiles covering the glacier\n",
    "    local_cnt = 0\n",
    "    for x in x_coords:\n",
    "        for y in y_coords:\n",
    "            # Tile definition of the box (in EPSG:2056)\n",
    "            tile_box = box(x, y, x + TILE_SIZE_M, y + TILE_SIZE_M) \n",
    "            \n",
    "            # Filter\n",
    "            if not tile_box.intersects(geom_shapely): continue\n",
    "            if tile_box.intersection(geom_shapely).area < MIN_INTERSECTION_M2: continue # Minimum intersection area explained before\n",
    "            \n",
    "            filename = os.path.join(OUTPUT_DIR, f\"{g_id}_{year}_tile_{local_cnt}.tif\")\n",
    "            if os.path.exists(filename): \n",
    "                local_cnt += 1\n",
    "                continue\n",
    "            \n",
    "            # EPSG:2056 -> EPSG:4326 (required in GEE :( )\n",
    "            box_gdf = gpd.GeoSeries([tile_box], crs=2056)\n",
    "            box_wgs84 = box_gdf.to_crs(epsg=4326).iloc[0]\n",
    "            \n",
    "            # We can use the box bounds directly to define the region in GEE\n",
    "            region_ee = ee.Geometry.Rectangle(\n",
    "                [box_wgs84.bounds[0], box_wgs84.bounds[1], box_wgs84.bounds[2], box_wgs84.bounds[3]],\n",
    "                proj='EPSG:4326',\n",
    "                geodesic=False # we want straight lines\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # Save\n",
    "                geemap.download_ee_image(\n",
    "                    get_sentinel_raw(int(year)),\n",
    "                    filename=filename,\n",
    "                    region=region_ee,   # Area of interest\n",
    "                    scale=10,           # 10m pixels\n",
    "                    crs='EPSG:2056',    # EPSG:2056\n",
    "                    dtype='uint16'      \n",
    "                )\n",
    "                stats[\"tiles_kept\"] += 1\n",
    "                local_cnt += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur {g_id}: {e}\")\n",
    "                stats[\"errors\"] += 1\n",
    "\n",
    "print(f\"Tiles generated : {stats['tiles_kept']}\")\n",
    "print(f\"Errors encountered : {stats['errors']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc30c95",
   "metadata": {},
   "source": [
    "## Mask Generation\n",
    "\n",
    "Here, we want that for each of our tiles that we generated in the cell above, a new file with only the pixel belonging to glacier activated.\n",
    "\n",
    "Additionally, we added a class_ignore index to fill the tiles: Why? Because neighbouring glacier have not always been labeled at the same year! this means that we can have a glacier that was labeled in 2016 on the left of our image, and a glacier labeled in 2018 on the right, where the boundaries are not the same as before! Thus, when we generate masks on a tile, if there is a difference in more than 1 year, we fill the other glacier with 255 -> ignored by the model (so it doesn't get punished or rewared on them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMAGE_DIR = \"dataset/images_raw_2056\"\n",
    "MASK_DIR = \"dataset/masks\"\n",
    "SHP_PATH = \"glamos/SGI_2016_wgs84.shp\"\n",
    "\n",
    "os.makedirs(MASK_DIR, exist_ok=True)\n",
    "\n",
    "# Year tolerance\n",
    "YEAR_TOLERANCE = 1 \n",
    "\n",
    "# Classes definitions\n",
    "CLASS_BACKGROUND = 0\n",
    "CLASS_GLACIER = 1\n",
    "CLASS_IGNORE = 255\n",
    "\n",
    "# Project shapefile to EPSG:2056 (LV95)\n",
    "gdf = gpd.read_file(SHP_PATH)\n",
    "gdf = gdf.set_crs(epsg=4326, allow_override=True).to_crs(epsg=2056)\n",
    "if gdf.crs.to_epsg() != 2056:\n",
    "    gdf = gdf.to_crs(epsg=2056)\n",
    "\n",
    "gdf[\"pk_glacier\"] = gdf[\"pk_glacier\"].astype(str)\n",
    "\n",
    "# Spatial index for fast queries\n",
    "sindex = gdf.sindex\n",
    "\n",
    "# List all the generated images\n",
    "image_files = [f for f in os.listdir(IMAGE_DIR) if f.endswith('.tif')]\n",
    "total = len(image_files)\n",
    "print(f\"Generating masks for {total} images\")\n",
    "\n",
    "for filename in tqdm(image_files):\n",
    "    \n",
    "    # Based on the filename we stored, we can recover the glacier ID and the year\n",
    "    try:\n",
    "        parts = filename.replace('.tif', '').split('_')\n",
    "        # format : ID_COMPLEX_YEAR_tile_NUMBER\n",
    "        target_year = int(parts[-3]) \n",
    "        target_id = \"_\".join(parts[:-3])\n",
    "    except Exception as e:\n",
    "        print(f\"   Filename error: {filename} -> {e}\")\n",
    "        continue\n",
    "    \n",
    "    # The paths are \"symmetric\" to the images     \n",
    "    image_path = os.path.join(IMAGE_DIR, filename)\n",
    "    mask_path = os.path.join(MASK_DIR, filename) \n",
    "    \n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Spatial info\n",
    "        out_shape = (src.height, src.width) # Expected (224, 224)\n",
    "        transform = src.transform \n",
    "        bounds = src.bounds \n",
    "        \n",
    "        # We recover the bounding box of the image, which we will then use to filter glaciers\n",
    "        bbox_geom = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "        \n",
    "        # Filter by spatial index (fast due to the spatail index that was built)\n",
    "        possible_matches_index = list(sindex.query(bbox_geom))\n",
    "        possible_matches = gdf.iloc[possible_matches_index]\n",
    "        \n",
    "        # Filter by intersection\n",
    "        visible_glaciers = possible_matches[possible_matches.intersects(bbox_geom)].copy()\n",
    "        \n",
    "        # Empty mask (full black image by default)\n",
    "        mask = np.zeros(out_shape, dtype=np.uint8)\n",
    "\n",
    "        if not visible_glaciers.empty:\n",
    "            # Check acquisition year\n",
    "            visible_glaciers['year_diff'] = (visible_glaciers['year_acq'] - target_year).abs()\n",
    "            \n",
    "            is_target = visible_glaciers['pk_glacier'].astype(str) == str(target_id)\n",
    "            is_good_year = visible_glaciers['year_diff'] <= YEAR_TOLERANCE\n",
    "            \n",
    "            valid_mask = is_target | is_good_year # target glacier or neighbor within tolerance\n",
    "            \n",
    "            bad_glaciers = visible_glaciers[~valid_mask]  # check neighbors too old/recent\n",
    "            good_glaciers = visible_glaciers[valid_mask]  # target year + recent neighbors\n",
    "            \n",
    "            # Rasterisation with the ignore class (turning the geometries into raster masks)\n",
    "            if not bad_glaciers.empty:\n",
    "                shapes_bad = ((geom, CLASS_IGNORE) for geom in bad_glaciers.geometry)\n",
    "                features.rasterize(\n",
    "                    shapes=shapes_bad,\n",
    "                    out=mask,\n",
    "                    transform=transform,\n",
    "                    default_value=CLASS_IGNORE,\n",
    "                    dtype=np.uint8\n",
    "                )\n",
    "                \n",
    "            # Rasterisation with the glacier that match the criteria\n",
    "            if not good_glaciers.empty:\n",
    "                shapes_good = ((geom, CLASS_GLACIER) for geom in good_glaciers.geometry)\n",
    "                features.rasterize(\n",
    "                    shapes=shapes_good,\n",
    "                    out=mask,\n",
    "                    transform=transform,\n",
    "                    default_value=CLASS_GLACIER,\n",
    "                    dtype=np.uint8\n",
    "                )\n",
    "        \n",
    "        # Save just 1 band, in UINT8\n",
    "        meta = src.meta.copy()\n",
    "        meta.update({\n",
    "            \"count\": 1,\n",
    "            \"dtype\": 'uint8',\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"compress\": \"lzw\",\n",
    "            \"nodata\": 0 # back = 0\n",
    "        })\n",
    "        \n",
    "        with rasterio.open(mask_path, 'w', **meta) as dst:\n",
    "            dst.write(mask, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ced75",
   "metadata": {},
   "source": [
    "## Corrupted Images (fully black)\n",
    "Some images resulted in being fully black! We filter them out of the dataset with the following cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e65aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SRC_IMG_DIR = \"dataset/images_raw_2056\"\n",
    "SRC_MASK_DIR = \"dataset/masks\"\n",
    "\n",
    "CLEAN_IMG_DIR = \"dataset/clean/images\"\n",
    "CLEAN_MASK_DIR = \"dataset/clean/masks\"\n",
    "\n",
    "os.makedirs(CLEAN_IMG_DIR, exist_ok=True)\n",
    "os.makedirs(CLEAN_MASK_DIR, exist_ok=True)\n",
    "\n",
    "files = [f for f in os.listdir(SRC_IMG_DIR) if f.endswith('.tif')]\n",
    "corrupted_count = 0\n",
    "valid_count = 0\n",
    "\n",
    "# Clean process\n",
    "\n",
    "for f in tqdm(files):\n",
    "    src_img_path = os.path.join(SRC_IMG_DIR, f)\n",
    "    src_mask_path = os.path.join(SRC_MASK_DIR, f)\n",
    "    \n",
    "    # Check mask existence\n",
    "    if not os.path.exists(src_mask_path):\n",
    "        corrupted_count += 1\n",
    "        continue\n",
    "\n",
    "    is_valid = False\n",
    "    \n",
    "    # Check contents (not empty)\n",
    "    try:\n",
    "        with rasterio.open(src_img_path) as src:\n",
    "            \n",
    "            if src.read().max() > 0:\n",
    "                is_valid = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error {f}: {e}\")\n",
    "    \n",
    "    if is_valid:\n",
    "        shutil.copy2(src_img_path, os.path.join(CLEAN_IMG_DIR, f))\n",
    "        shutil.copy2(src_mask_path, os.path.join(CLEAN_MASK_DIR, f))\n",
    "        valid_count += 1\n",
    "    else:\n",
    "        corrupted_count += 1\n",
    "\n",
    "\n",
    "print(f\"Rejected images : {corrupted_count}\")\n",
    "print(f\"Valid images : {valid_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2c8c5",
   "metadata": {},
   "source": [
    "Half of the data is gone, but this is the cost of a high quality dataset. To visualize what's left of the data, here we filter the original shapefile with only the glacier for which we have some images. (Only useful for visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d827e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_IMG_DIR = \"dataset/clean/images\"\n",
    "ORIGINAL_SHP = \"glamos/SGI_2016_wgs84.shp\"\n",
    "OUTPUT_SHP = \"glamos/SGI_2016_VALID_ONLY.shp\"\n",
    "\n",
    "\n",
    "files = [f for f in os.listdir(CLEAN_IMG_DIR) if f.endswith('.tif')]\n",
    "if not files:\n",
    "    raise ValueError(\"Erreur : Le dossier d'images est vide !\")\n",
    "\n",
    "# IDs extraction - expected format : ID_YEAR_tile_X.tif\n",
    "valid_ids = set()\n",
    "for f in tqdm(files):\n",
    "    parts = f.replace('.tif', '').split('_')\n",
    "    if len(parts) >= 3:\n",
    "        g_id = \"_\".join(parts[:-3])\n",
    "        valid_ids.add(g_id)\n",
    "print(f\" Unique IDs: {len(valid_ids)} \")\n",
    "\n",
    "gdf = gpd.read_file(ORIGINAL_SHP)\n",
    "# Convert ID into string\n",
    "gdf['pk_glacier'] = gdf['pk_glacier'].astype(str)\n",
    "# Filter\n",
    "gdf_valid = gdf[gdf['pk_glacier'].isin(valid_ids)].copy()\n",
    "print(f\" {len(gdf_valid)} polygones kept over {len(gdf)} \")\n",
    "# Save\n",
    "gdf_valid.to_file(OUTPUT_SHP)\n",
    "print(\"Filtered shapefile saved to\", OUTPUT_SHP,\"Can be used on QGIS to see which of our Glacier are not trown out.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc698cf8",
   "metadata": {},
   "source": [
    "## Test Set isolation\n",
    "Using the new shapefile that the previous cell generated, we used QGIS to create a list of glacier (spacially isolated) to be used as a test set, that we will move in another folder here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRY_RUN = False  # small security just to do the checks\n",
    "TSV_PATH = \"dataset/test_set_idx.tsv\"\n",
    "SRC_IMG_DIR = \"dataset/clean/images\"\n",
    "SRC_MASK_DIR = \"dataset/clean/masks\"\n",
    "DEST_IMG_DIR = \"dataset/test/images\"\n",
    "DEST_MASK_DIR = \"dataset/test/masks\"\n",
    "\n",
    "if not DRY_RUN:\n",
    "    os.makedirs(DEST_IMG_DIR, exist_ok=True)\n",
    "    os.makedirs(DEST_MASK_DIR, exist_ok=True)\n",
    "\n",
    "# We retreive the IDs of the glacier using the TSV that we exported from QGIS\n",
    "test_set = pd.read_csv(TSV_PATH, sep=\"\\t\")\n",
    "target_ids = set(test_set[\"pk_glacier\"].astype(str).str.strip().str.replace('\"', ''))\n",
    "files = [f for f in os.listdir(SRC_IMG_DIR) if f.endswith('.tif')]\n",
    "\n",
    "\n",
    "files_to_move = []\n",
    "ids_found_in_files = set()\n",
    "# We just do a greedy match \n",
    "for filename in tqdm(files, desc=\"Matching test set IDs\"):\n",
    "    parts = filename.replace('.tif', '').split('_')\n",
    "    if len(parts) >= 3:\n",
    "        file_glacier_id = \"_\".join(parts[:-3])\n",
    "        # ID found in target??\n",
    "        if file_glacier_id in target_ids:\n",
    "            files_to_move.append(filename)\n",
    "            ids_found_in_files.add(file_glacier_id)\n",
    "\n",
    "#Compute stats\n",
    "missing_ids = target_ids - ids_found_in_files\n",
    "total_tiles = len(files_to_move)\n",
    "\n",
    "print(f\"Results Dry Run simulation: {DRY_RUN})\")\n",
    "print(f\"Target : {len(target_ids)}\")\n",
    "print(f\"Glaciers found: {len(ids_found_in_files)}\")\n",
    "print(f\"Missing glaciers: {len(missing_ids)}\")\n",
    "print(f\"Total tiles: {total_tiles}\")\n",
    "\n",
    "if len(missing_ids) > 0:\n",
    "    print(f\"Examples of missing IDs: {list(missing_ids)[:5]}\")\n",
    "    print(\"Glaciers found in target but no matching images in clean\")\n",
    "\n",
    "if total_tiles > 0:\n",
    "    if DRY_RUN:\n",
    "        print(f\"\\ntest: {total_tiles} files\")\n",
    "    else:\n",
    "        moved_count = 0\n",
    "        for filename in tqdm(files_to_move, desc=\"Déplacement\"):\n",
    "            src_img = os.path.join(SRC_IMG_DIR, filename)\n",
    "            src_msk = os.path.join(SRC_MASK_DIR, filename)\n",
    "            \n",
    "            dst_img = os.path.join(DEST_IMG_DIR, filename)\n",
    "            dst_msk = os.path.join(DEST_MASK_DIR, filename)\n",
    "            \n",
    "            try:\n",
    "                shutil.move(src_img, dst_img)\n",
    "                \n",
    "                if os.path.exists(src_msk):\n",
    "                    shutil.move(src_msk, dst_msk)\n",
    "                moved_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {filename}: {e}\")\n",
    "                \n",
    "        print(f\"\\n{moved_count} tiles in dataset/test\")\n",
    "else:\n",
    "    print(\"\\nNothing to move\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPEO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
